{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Google image embeddings","metadata":{}},{"cell_type":"code","source":"!pip install timm\nimport timm","metadata":{"execution":{"iopub.status.busy":"2022-08-11T06:34:48.019462Z","iopub.execute_input":"2022-08-11T06:34:48.020520Z","iopub.status.idle":"2022-08-11T06:35:01.983152Z","shell.execute_reply.started":"2022-08-11T06:34:48.020414Z","shell.execute_reply":"2022-08-11T06:35:01.981861Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset \nfrom torch.utils.data import DataLoader\n\nimport torchvision.models as models\nfrom torchvision import transforms as tf\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport os\nfrom pathlib import Path\nfrom zipfile import ZipFile\nfrom tqdm.notebook import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2022-08-11T06:35:01.985183Z","iopub.execute_input":"2022-08-11T06:35:01.985515Z","iopub.status.idle":"2022-08-11T06:35:05.345338Z","shell.execute_reply.started":"2022-08-11T06:35:01.985484Z","shell.execute_reply":"2022-08-11T06:35:05.344224Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class EmbeddingsDataset(Dataset):\n    def __init__(self, size=[256, 256], aug_transforms=None, max_elem_per_class=None):\n        self.images = []\n        self.augmentation = aug_transforms\n        self.max_elem_per_class = max_elem_per_class\n        \n        self.basic_transforms = tf.Compose([\n            tf.Resize(size),\n            tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        \n        path = \"../input/130k-images-512x512-universal-image-embeddings\"\n        dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n        \n        for folder in dirs:\n            tree=os.walk(path+\"/\"+folder, topdown=True)\n            folder_size = len(next(tree)[2])\n            for i in tqdm(range(folder_size if self.max_elem_per_class is None else self.max_elem_per_class)):\n                img_path = path+\"/\"+folder+\"/\"+\"image\"+\"{:04d}\".format(i)+\".jpeg\"\n                print(img_path)\n                image = cv2.imread(img_path)\n                image = torch.tensor(image).permute((2, 0, 1))[None,:,:,:] / 255\n                image = self.basic_transforms(image)\n                self.images += [image]\n        \n    \n    def __getitem__(self, idx):\n        return images[idx]\n    \n    def __len__(self):\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-08-11T06:36:31.582039Z","iopub.execute_input":"2022-08-11T06:36:31.583326Z","iopub.status.idle":"2022-08-11T06:36:31.602836Z","shell.execute_reply.started":"2022-08-11T06:36:31.583274Z","shell.execute_reply":"2022-08-11T06:36:31.601155Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import glob, os\nos.chdir(\"../input/130k-images-512x512-universal-image-embeddings/artwork\")\nfor file in glob.glob(\"*.jpg\"):\n    print(file)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T06:33:42.652421Z","iopub.execute_input":"2022-08-11T06:33:42.652841Z","iopub.status.idle":"2022-08-11T06:33:42.668886Z","shell.execute_reply.started":"2022-08-11T06:33:42.652806Z","shell.execute_reply":"2022-08-11T06:33:42.667233Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset = EmbeddingsDataset()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T06:36:35.189787Z","iopub.execute_input":"2022-08-11T06:36:35.190184Z","iopub.status.idle":"2022-08-11T06:36:37.185440Z","shell.execute_reply.started":"2022-08-11T06:36:35.190153Z","shell.execute_reply":"2022-08-11T06:36:37.183854Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"image\" + \"{:04d}\".format(100) + \".jpg\"\n\ntree=os.walk(\"../input/130k-images-512x512-universal-image-embeddings/apparel\", topdown=True)\nprint(len(next(tree)[2])) \n\npath = \"../input/130k-images-512x512-universal-image-embeddings\"\ndirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\ndirs\n\nartwork_path = \"../input/130k-images-512x512-universal-image-embeddings\"+\"/\"+dirs[0]+\"/\"\nimgs = [d for d in os.listdir(artwork_path) if os.path.isdir(os.path.join(artwork_path, d))]\nlen(imgs)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T06:08:43.683864Z","iopub.execute_input":"2022-08-11T06:08:43.684798Z","iopub.status.idle":"2022-08-11T06:09:08.670659Z","shell.execute_reply.started":"2022-08-11T06:08:43.684762Z","shell.execute_reply":"2022-08-11T06:09:08.669824Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, torch.nn.Linear) :\n        torch.nn.init.xavier_uniform_(m.weight)\n        \n    if isinstance(m, torch.nn.ConvTranspose2d) or isinstance(m, torch.nn.Conv2d):\n        torch.nn.init.xavier_normal_(m.weight.data)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:08:53.304593Z","iopub.execute_input":"2022-08-11T04:08:53.305060Z","iopub.status.idle":"2022-08-11T04:08:53.312474Z","shell.execute_reply.started":"2022-08-11T04:08:53.305027Z","shell.execute_reply":"2022-08-11T04:08:53.311339Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Embedder(torch.nn.Module):\n    def __init__(self, model_name, target_size=[384, 384], emb_size=64):\n        super().__init__()\n        \n        self.target_size = target_size\n        self.emb_size = emb_size\n        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0) \n        self.backbone.requires_grad = False\n        \n        self.head = torch.nn.Sequential(\n            torch.nn.Linear(1536, 1024),\n            torch.nn.Linear(1024, self.emb_size)\n        )\n        self.head.apply(weights_init)\n\n        \n    def forward(self, x):\n        x = torch.tensor(x).permute((2, 0, 1))[None,:,:,:] \n        \n        x = tf.functional.resize(x, size=self.target_size)\n        \n        if not self.training:\n            x = x / 255.\n            x = tf.functional.normalize(x, \n                                        mean=[0.485, 0.456, 0.406], \n                                        std=[0.229, 0.224, 0.225])\n            \n        x = self.backbone(x)\n        x = self.head(x)\n        \n        return x\n        \n        \n    def params_count(self):\n        print(\"Params in backbone: \", sum(p.numel() for p in self.backbone.parameters()))\n        print(\"Params in head: \", sum(p.numel() for p in self.head.parameters()))\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:13:27.666275Z","iopub.execute_input":"2022-08-11T04:13:27.667780Z","iopub.status.idle":"2022-08-11T04:13:27.682576Z","shell.execute_reply.started":"2022-08-11T04:13:27.667716Z","shell.execute_reply":"2022-08-11T04:13:27.681522Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model = Embedder(\"swin_large_patch4_window12_384_in22k\")\nmodel.params_count()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:13:28.084911Z","iopub.execute_input":"2022-08-11T04:13:28.085893Z","iopub.status.idle":"2022-08-11T04:13:32.111818Z","shell.execute_reply.started":"2022-08-11T04:13:28.085835Z","shell.execute_reply":"2022-08-11T04:13:32.110487Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread(\"../input/130k-images-512x512-universal-image-embeddings/landmark/image0000.jpg\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:10:19.093718Z","iopub.execute_input":"2022-08-11T04:10:19.094187Z","iopub.status.idle":"2022-08-11T04:10:19.381048Z","shell.execute_reply.started":"2022-08-11T04:10:19.094152Z","shell.execute_reply":"2022-08-11T04:10:19.379673Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"rez = model(img)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:10:20.240189Z","iopub.execute_input":"2022-08-11T04:10:20.241454Z","iopub.status.idle":"2022-08-11T04:10:23.148021Z","shell.execute_reply.started":"2022-08-11T04:10:20.241377Z","shell.execute_reply":"2022-08-11T04:10:23.146609Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"rez","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:10:28.244631Z","iopub.execute_input":"2022-08-11T04:10:28.245106Z","iopub.status.idle":"2022-08-11T04:10:28.256216Z","shell.execute_reply.started":"2022-08-11T04:10:28.245071Z","shell.execute_reply":"2022-08-11T04:10:28.254867Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model.eval()\nsaved_model = torch.jit.script(model)\nsaved_model.save('saved_model.pt')\n\nwith ZipFile('submission.zip','w') as zip:           \n    zip.write('./saved_model.pt', arcname='saved_model.pt') ","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:18:39.823544Z","iopub.execute_input":"2022-08-11T03:18:39.824060Z","iopub.status.idle":"2022-08-11T03:18:45.956041Z","shell.execute_reply.started":"2022-08-11T03:18:39.824019Z","shell.execute_reply":"2022-08-11T03:18:45.954431Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}